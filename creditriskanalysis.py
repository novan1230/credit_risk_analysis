# -*- coding: utf-8 -*-
"""Salinan dari CreditRiskAnalysis_idx.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bh9Syi-CIH6gxW3oH8oVCW8XA82Ad52m

# **Implementation of Machine Learning in Credit Risk Analysis: Predicting Potential Customer Defaults**

**by: Novan Rizki Wicaksono**

# **Business Understanding**

### **üéØ Background of the Problem**
1. **Credit risk** is a **crucial element** that needs to be understood by financial institutions and loan service providers. To optimize the management of this risk, finance companies continue to strive to **improve creditworthiness assessment methods**. Approving loans to borrowers with a high risk of default can have a negative impact on the company's financial performance, and errors in transmitting credit risk have the potential to have serious consequences.

2. In the world of finance, a suboptimal credit granting process and inappropriate credit risk management can result in major losses. By applying **machine learning algorithms** to analyze large amounts of historical data, financial institutions can **uncover patterns and trends** that are difficult for human analysts to detect. This allows for **more accurate** credit decision making and **more efficient risk management**, thereby **minimizing potential losses** due to inappropriate credit granting.

3. Therefore, the development of a machine learning model that is able to predict potential defaults based on factors such as the customer's economic and financial conditions is very important to **avoid losses due to bad debts**.
<br>


### **üß† Business Problem**
1. How to **identify the risk of customer** default through historical data analysis to **improve the accuracy of credit** assessment by companies, so that companies can make more informed lending decisions?

2. **11% of 460k** customers still in default

<br>

### **üéØ Project Objective**
Design a **predictive model using machine learning** that can categorize borrowers into two main categories, namely:

    ‚úÖ GOOD ‚Üí Low risk

    ‚ùå BAD ‚Üí High risk

<br>

### **üìå Business Benefits**

1.   üìâ Mitigate bad credit risk
2.   ‚è±Ô∏è Streamlining credit approval process
3.   üßÆ Data analysis-based credit policy formulation
4.   ‚ú® Minimize potential losses due to bad debts
5.   üîç Optimize creditworthiness screening process
6.   üìà Support the preparation of more precise credit granting regulations

<br>

### **üè¶ Data Context**
This project was developed to **build a predictive system to assess credit risk **by utilizing **Lending Club's historical database**. The main objective of this project is to **create a predictive solution** to **improve the accuracy of credit assessment**, especially in detecting potential default risk customers from the early stages of the process.

The **dataset** used covers the **period 2007-2014** and contains information such as:
*   üìä Loan Amount
*   üìà Interest Rate
*   üíµ Annual Income
*   ‚úÖ‚ùå Payment Status

<br>

### **‚ùì Problem Statement**
How to **build a prediction model** that can **classify loan risk based on historical data**, so that companies can **reduce the potential for non-performing loans**?

# **Data Understanding**


## **Data Collection**

### **Import Library**
"""

# Data manipulation
import pandas as pd
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', 99)
import numpy as np
import datetime
import matplotlib.font_manager as fm
from matplotlib.ticker import FuncFormatter
import matplotlib.pyplot as plt
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['font.sans-serif'] = ['Arial', 'DejaVu Sans', 'Helvetica']

# Normality test
from scipy.stats import chi2_contingency
from scipy.stats import shapiro
from scipy.stats import normaltest
import scipy.stats as st

# Data visualization
import matplotlib as mpl
import matplotlib.pyplot as plt
from matplotlib import rcParams
import seaborn as sns

# Machine Learning
import sklearn
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import MinMaxScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
import scipy
from scipy import stats
from scipy.stats import skew
from imblearn.under_sampling import RandomUnderSampler
from sklearn.model_selection import train_test_split

# Models
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from lightgbm import LGBMClassifier
from xgboost import XGBClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB

#Feature Encoding
from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer

#CrossValidate
from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, roc_auc_score
from sklearn.model_selection import cross_validate
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import StratifiedKFold, KFold

#Hypertuning Parameter
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from sklearn.metrics import roc_auc_score, roc_curve, RocCurveDisplay, precision_recall_curve, PrecisionRecallDisplay
from sklearn import metrics
from sklearn.metrics import confusion_matrix

# Evaluation Metrics
from sklearn.metrics import accuracy_score, auc, roc_auc_score, precision_score, recall_score, f1_score, fbeta_score
from sklearn.metrics import balanced_accuracy_score, classification_report, confusion_matrix
from sklearn.model_selection import learning_curve, cross_val_score

# Ignore Warning
import warnings
warnings.filterwarnings('ignore')

"""### **Load Dataset**"""

from matplotlib import rcParams

# Configure default font settings
rcParams['font.family'] = 'sans-serif'  # Set default font family

# Set default figure dimensions (width, height in inches)
rcParams['figure.figsize'] = (12, 8)

# Customize plot line appearance
rcParams['lines.linewidth'] = 3  # Default line thickness

# Configure title and label sizes
rcParams['axes.titlesize'] = 'x-large'  # Axis title size
rcParams['xtick.labelsize'] = 'medium'  # X-axis tick label size
rcParams['ytick.labelsize'] = 'medium'  # Y-axis tick label size

# Set axes background color (white)
rcParams['axes.facecolor'] = '1'  # Using numeric code for white

df_idx = pd.read_csv("loan_data_2007_2014.csv")
df_idx

df_idx.info()

# Descriptive Statistic

df_idx.describe()

# Drop Unnamed: 0 column

if 'Unnamed: 0' in df_idx.columns:
    df_idx.drop('Unnamed: 0', axis=1, inplace=True)

"""### **Duplicate Data Check**"""

print("Data dimension: {} rows and {} column".format(*df_idx.shape))
print("Unique value of id column: {}".format(df_idx.id.nunique()))
print("Unique value of member_id column: {}".format(df_idx.member_id.nunique()))

print('There are', df_idx.duplicated().sum(), 'duplicated value')

"""After checking, it turns out that there is no duplicate data. This means that each row in the dataset represents one individual entity.

### **Drop Null Column**
"""

# Identify and sort columns based on the number of empty data
df_idx.isnull().sum().sort_values(ascending=False)

columns_to_drop = [
    # unique values
    'id','member_id',

    # all null values
    'inq_last_12m', 'verification_status_joint', 'open_acc_6m', 'open_il_6m', 'open_il_12m', 'open_il_24m',
    'mths_since_rcnt_il', 'dti_joint', 'total_bal_il', 'annual_inc_joint', 'il_util', 'open_rv_12m', 'open_rv_24m',
    'max_bal_bc', 'all_util', 'inq_fi', 'total_cu_tl',

    # free text
    'url', 'desc',

    # censored personal value (e.g. 330xxx)
    'zip_code',

    # detailed variables (already represented by the `grade` column)
    'sub_grade',

    # constant values
    'policy_code']

# Find the column that has all null/NA values

fully_null = []
for column in df_idx.columns:
    if df_idx[column].isnull().all():
        fully_null.append(column)

print("Column without data:", fully_null)
print(f"Find {len(fully_null)} Column without data/values")

df = df_idx.drop(columns_to_drop, axis=1)
df_idx.shape

df_idx.info()

"""# **Data Preparation**

### **Labeling**

Credit risk refers to the potential failure of a customer to fulfill loan repayment obligations as agreed with the lender. Credit risk modeling projects aim to predict the probability of a borrower repaying their loan obligations. Therefore, the target variables chosen must accurately represent the repayment capacity of the potential borrower. As a form of risk management, lenders conduct creditworthiness analysis, historical checks of previous payments, credit score evaluation, and assessment of the borrower's financial capability.
<br>

The `loan_status` feature in the dataset is considered ideal as a target variable because it reflects the borrower's actual payment history, shows historical payment performance, and provides an indication of the ability to fulfill credit obligations. In the context of this analysis, `loan_status` is an appropriate target parameter because this variable:
1.   Records the repayment status of each individual
2.   Reflects the consistency of loan repayments
3.   Is a direct indicator of the borrower's credit behavior
"""

# Analyze loan status distribution
print("Loan Status Distribution:")
print(df['loan_status'].value_counts())

"""*   Current: Customer consistently pays installments before/after due date
*   Fully Paid: Principal and interest have been fully repaid
*   Charged Off: Non-performing loans that have been written off after collection efforts
*   Late: Payment exceeds the grace period (usually >30 days late)
*   In Grace Period: A tolerance period of 1-15 days after maturity without penalty.
*   Default: Default after a certain period (usually >90-120 days)

<br>

Based on this definition, **good loan** and **bad loan** customers can be defined. A loan is categorized as a **good loan** if all payment obligations are fulfilled on time according to the agreement, while a **bad loan** refers to a loan that has significant payment delays or even defaults.

"""

#  List of loan statuses that are considered good (not in default)
good_loan_status = ['Current', 'Fully Paid', 'Does not meet the credit policy. Status:Fully Paid', 'In Grace Period', 'Late (16-30 days)']

# Create a default probability column: 0 = no default, 1 = default
df['proba_of_default'] = np.where(df['loan_status'].isin(good_loan_status), 0, 1)

# After creating the proba_of_default column, analyze its distribution
percentage_default = df['proba_of_default'].value_counts(normalize=True).mul(100).round(4)
print(f"\nPortfolio Composition:\n{percentage_default}%")

loan_dist = df['proba_of_default'].value_counts(normalize=True) * 100
labels = ['Good Loan', 'Bad Loan']
colors = ['#66b3ff','#ff9999']

plt.figure(figsize=(8,6))
plt.pie(loan_dist, labels=labels, colors=colors, autopct='%.1f%%', startangle=90, explode=(0.05, 0))
plt.title('Good Loan vs Bad Loan Distribution', fontweight='bold')
plt.axis('equal')
plt.show()

"""**Good Loan (0):**
*   Has been repaid according to the provisions
*   Does not violate credit policy
*   Status: Paid off

**Bad Loan (1) :**
*   Charged-off
*   Does not meet credit requirements
*   Status: Collected, Defaulted, or Delinquent (31-120 days)

**Key distinction:**
<br>
Current loans indicate timely payments and fulfillment of obligations, while non-performing loans include defaults, collections, or significant delays.
"""

df["loan_status"].unique()

"""# **Data Preprocessing**

### **Feature Engineering**

`Term`
<br>
*   Term credit is commonly used in the context of long-term credit, such as mortgages or car loans, where the loan amount is relatively large and the repayment term is several years.
*   This feature represents the number of months that the debtor/borrower has to keep making payments.
*   The value of this feature will be converted to an integer, for example, 36 months -> 36.
"""

df["term"].unique()

# Safely convert term to integer after cleaning string
df['term_int'] = (df['term'].str.extract('(\d+)')[0]  # Extract digits
                  .astype(int))  # Convert to integer

df[['term', 'term_int']].sample(3, random_state=42)

"""`emp_length`

The employment length feature **represents the duration of the borrower's work** experience in years. In credit analysis, longer tenure is generally correlated with better job stability and a reduced level of credit risk. The value in this feature will be converted to numeric form, for example ‚Äú15+ years‚Äù will be transformed into 15.



"""

df['emp_length'].unique()

df['emp_length_int'] = (df['emp_length']
                       .str.replace('\+ years', '')
                       .str.replace('< 1 year', '0')
                       .str.replace(' years', '')
                       .str.replace(' year', ''))

# Convert to float after string cleaning
df['emp_length_int'] = pd.to_numeric(df['emp_length_int'], errors='coerce').astype(float)

df[['emp_length','emp_length_int']].sample(3)

"""**Snapshot Date**

Before making modifications to features that contain dates, it is important to set a *snapshot date* or reference date first. A *snapshot date* refers to a specific date when data was captured or ‚Äúrecorded‚Äù, which serves as a benchmark in analysis. All data recorded on or before this date is considered current.  

This dataset includes data from 2007 to 2014. If the current year is used as the *snapshot date*, the analysis results may be less relevant. In addition, the column `last_pymnt_d` indicates the last payment in early January 2016.  

In order for all data to be treated consistently, the *snapshot date* is set at the end of 2016, i.e. **2016-12-01**. Thus, all calculations and analysis will refer to that date.
"""

snapshot_date = pd.to_datetime('2016-12-01')
snapshot_date

"""`issue_d`

This feature provides loan-related temporal data that is useful for analyzing lending patterns chronologically. This variable serves as the basis for calculating loan tenor and categorizing financing products. For analytical purposes, the feature will be converted to monthly duration relative to the reference date (snapshot date).

"""

df['issue_d'].unique()

df['date_issue_d'] = pd.to_datetime(df['issue_d'], format='%b-%y')
df['date_issue_d'].unique()

df['months_since_issue_d'] = (
    (snapshot_date - df['date_issue_d']).dt.days // 30  # Pembagian integer
).astype(int)

(
    df[['issue_d', 'date_issue_d', 'months_since_issue_d']]
    .pipe(lambda x: print(f"Snapshot Date: {snapshot_date}") or x)
    .sample(3)
    .pipe(display)
)

df['months_since_issue_d'].describe()

df.drop('emp_length', axis=1, inplace=True)

"""`earliest_cr_line`

This feature quantifies the duration of a borrower's experience in managing financial obligations. Specifically, this variable is used to measure the **credit history length** at the time of loan application. Therefore, we will transform it into months relative to the reference date (**snapshot date**).

"""

df['earliest_cr_line'].head(3)

"""Based on the output above, it can be assumed that the last two numbers are years. Where ‚Äú-99‚Äù means 1999 and ‚Äú-01‚Äù means 2001."""

df['earliest_cr_line_date'] = pd.to_datetime(df['earliest_cr_line'], format='%b-%y')
df['earliest_cr_line_date'].head(3)

df['months_since_earliest_cr_line'] = round(
    (pd.to_datetime('2017-12-01') - df['earliest_cr_line_date']).dt.days / 30
)

"""**or**"""

#If the previous code is in error, then you can use this code

#df_idx['months_since_earliest_cr_line'] = round((snapshot_date - df_idx['date_earliest_cr_line']).dt.days / 30.44)  # Avg day per month

# Create a subset dataframe containing columns related to earliest credit line
credit_line_data = df[['earliest_cr_line', 'earliest_cr_line_date', 'months_since_earliest_cr_line']]

# Print the snapshot date and display the first 3 rows of the subset
print('Current snapshot date:', snapshot_date)
display(credit_line_data.head(3))

df['months_since_earliest_cr_line'].describe()

"""There is a negative value in min **(-621)**"""

# Display records with negative months since earliest credit line (anomalies)
negative_months_data = credit_line_data[credit_line_data['months_since_earliest_cr_line'] < 0]

# Show the first 3 problematic records along with their total count
display(negative_months_data.head(3), negative_months_data.count())

"""**Root Cause of Negative Values**
<br>

The negative values in the dataset stem from a date parsing issue:
*  Python incorrectly interpreted the two-digit year '62' as 2062 instead of 1962, leading to invalid month calculations.

<br>

**Resolution Approach**

**1.   Potential Solution:**
  *   A deeper preprocessing step could correct the year (2062 ‚Üí 1962), but this was not implemented here.
<br>

**2.   Applied Fix:**
*   Negative values were replaced with the average value of the feature, as they represent historically old records (1900s).
*   This approach is justified because:
    *   The affected records are extremely old (likely from inactive customers).
    *   Market dynamics have shifted significantly since the 20th‚Äì21st century transition, reducing their relevance.
"""

# Display original dataset dimensions before removing invalid records
print('Dataset shape before removal:', df.shape)

# Remove rows with negative values in 'months_since_earliest_cr_line'
# (These represent date parsing errors where years were misinterpreted)
df.drop(df[df['months_since_earliest_cr_line'] < 0].index, inplace=True)

# Display updated dataset dimensions after cleaning
print('Dataset shape after removal:', df.shape)

# Alternative approach using query() (commented out for reference)
# df = df.query('months_since_earliest_cr_line >= 0')

df['months_since_earliest_cr_line'].describe()

"""Based on the previous output, the `months_since_earliest_cr_line` column is now free of invalid values (negative values). Thus, the entire data modification process has been successfully completed, and the dataset is now in a consistent and safe condition to be used in further analysis.

"""

df.drop(['earliest_cr_line'], axis=1, inplace=True)

"""`last_pymnt_d`

This feature records the most recent period (month and year) when the borrower made credit payments. This data has important analytical value as it can reflect the borrower's payment behavior, including the propensity to meet obligations in a timely manner.

For more effective analysis, we will change the format of this feature to:
1.   Time difference in months
2.   Measured relative to the snapshot date
3.  With a clear numerical representation

This transformation will result in a ‚Äúnumber of months since last payment‚Äù metric that is easier to interpret and provides a more accurate picture of:
1.   Recency of payment activity
2.   Regularity of payment pattern
3.   Risk of late payment

This change in format allows for more precise analysis in evaluating borrowers' repayment habits.
"""

df['last_pymnt_d'].unique()

df['date_last_pymnt_d'] = pd.to_datetime(df['last_pymnt_d'], format='%b-%y')
df['date_last_pymnt_d'].unique()

from dateutil.relativedelta import relativedelta

df['months_since_last_pymnt_d'] = df['date_last_pymnt_d'].apply(
    lambda x: (snapshot_date.year - x.year) * 12 + (snapshot_date.month - x.month)
)

# Create a new DataFrame containing only three specific columns related to payment dates
payment_date_info = df[['last_pymnt_d', 'date_last_pymnt_d', 'months_since_last_pymnt_d']]

# Print the reference snapshot date used for calculations
print('Reference Date Used for Calculations:', snapshot_date)

# Display the first 3 rows of the payment date information
display(payment_date_info.head(3))

df['last_pymnt_d'].describe()

"""Based on the output results displayed, the values in the `months_since_last_pymnt_d` column look valid and do not contain suspicious data. Thus, the data modification stage has been successfully carried out and the entire dataset is declared safe for further processing."""

df.drop(['last_pymnt_d'], axis=1, inplace=True)

"""`next_pymnt_d`

This feature represents the due date of the next payment that has been scheduled for the borrower. Although it cannot ensure whether the payment will actually be made, this feature still has significant predictive value. Analyze the time difference between:
* Due date
* Last payment date

may indicate potential payment delays or defaults. For analysis purposes, this feature will be converted into a duration format (in months) relative to a reference date (*snapshot date*).
"""

df['next_pymnt_d'].unique()

df['date_next_pymnt_d'] = pd.to_datetime(df['next_pymnt_d'], format='%b-%y')
df['date_next_pymnt_d'].unique()

from dateutil.relativedelta import relativedelta

df['months_since_next_pymnt_d'] = df['date_next_pymnt_d'].apply(
    lambda x: (snapshot_date.year - x.year) * 12 + (snapshot_date.month - x.month)
)

# Create a new dataframe that only contains columns related to the next payment due date
payment_due_info = df[['next_pymnt_d', 'date_next_pymnt_d', 'months_since_next_pymnt_d']]

# Display the reference date used in the calculation
print('Reference Date (Snapshot Date):', snapshot_date)

# Display the last 3 entries of the payment_due_info dataframe
# to check the most recent data in the dataset
display(payment_due_info.tail(4))

df.drop(['next_pymnt_d'], axis=1, inplace=True)

"""`last_credit_pull_d`

This feature records the date when the borrower's credit data was last updated by the credit bureau. This information has two main functions:

1. Becomes the main reference in credit assessment when applying for a new loan
2. Indicates how up-to-date the borrower's credit profile is

For analysis purposes, this feature will be converted into a time difference (in months) and measured relative to a reference date (`snapshot date`).
"""

df['last_credit_pull_d'].unique()

df['date_last_credit_pull_d'] = pd.to_datetime(df['last_credit_pull_d'], format='%b-%y')
df['date_last_credit_pull_d'].unique()

from dateutil.relativedelta import relativedelta

df['months_since_last_credit_pull_d'] = df['date_last_credit_pull_d'].apply(
    lambda x: (snapshot_date.year - x.year) * 12 + (snapshot_date.month - x.month)
)

# Create a new dataframe that only contains columns related to the next payment due date
payment_due_info = df[['last_credit_pull_d', 'date_last_credit_pull_d', 'months_since_last_credit_pull_d']]

# Display the reference date used in the calculation
print('Reference Date (Snapshot Date):', snapshot_date)

# Display the last 3 entries of the payment_due_info dataframe
# to check the most recent data in the dataset
display(payment_due_info.tail(4))

df['months_since_last_credit_pull_d'].describe()

"""**Data Verification Results**
<br>

Based on the output check, all values in the `months_since_last_credit_pull_d` column have met the following criteria:
1. Data Validity: Does not contain negative values, missing values, or suspicious outliers
2. Format Consistency: The time difference in months is accurately calculated against the snapshot date

**Implications**
<br>
The data transformation process is declared:
* Complete (no further modifications required)
* Secure (ready to be used for credit modeling or risk analysis)

"""

df.drop(['last_credit_pull_d'], axis=1, inplace=True)

df.sample(5)

"""# **Exploratory Data Analysis**

### **Outlier Handling**
"""

# Select all numerical columns from the dataframe
numerical_columns = df.select_dtypes(include=['int64', 'float64']).columns.tolist()

# Display the list of numerical features
print("Numerical Features in Dataset:")
print(numerical_columns)

# Print the count of numerical columns
print("\nTotal Number of Numerical Columns:", len(numerical_columns))

valid_features = [col for col in numerical_columns
                 if df[col].notna().any() and df[col].nunique() > 1]

figure, axes = plt.subplots(nrows=4, ncols=10, figsize=(20, 10))

for index, feature in enumerate(valid_features):  # Use filtered list
    row_position = index // 10
    col_position = index % 10

    sns.boxplot(x=df[feature], color='blue', ax=axes[row_position, col_position])
    axes[row_position, col_position].set_title(feature, fontsize=8)

# Hide unused subplots
for j in range(len(valid_features), 40):
    axes.flatten()[j].axis('off')

plt.tight_layout()
plt.show()

"""There are **3 columns** that have extreme outliers that deviate significantly from the normal distribution. These columns are `tot_coll_amt, tot_cur_bal, total_rev_hi_lim`. These columns will be reduced by manual outliers because many features are indeed outliers (very high variance)."""

from scipy import stats

# Display the dataframe dimension before outlier removal
print('Data Count Before Outlier Removal:', df.shape)

# Removing rows with extreme values in certain columns
# For column tot_coll_amt (total collections)
df.drop(df[df['tot_coll_amt'] > 9_000_000].index,
 inplace=True,
 errors='ignore')

# For tot_cur_bal column (current balance)
df.drop(df[df['tot_cur_bal'] > 7_000_000].index,
 inplace=True,
 errors='ignore')

# For column total_rev_hi_lim (highest credit limit)
df.drop(df[df['total_rev_hi_lim'] > 9_000_000].index,
 inplace=True,
 errors='ignore')

# Display the dimensions of the dataframe after removal
print('Total Data After Outlier Removal:', df.shape)

# Create a figure with 3 horizontal subplots (1 row x 3 columns)
plt.figure(figsize=(8, 4)) # Canvas size 8x4 inches

# Boxplot for total number of collections
plt.subplot(1, 3, 1) # First subplot position
sns.boxplot(data=df, x='tot_coll_amt', color='green')
plt.title('Total Collection', fontsize=10)
plt.xlabel('Value (in million)', fontsize=8)

# Boxplot for current balance
plt.subplot(1, 3, 2) # Second subplot position
sns.boxplot(data=df, x='tot_cur_bal', color='green')
plt.title('Current Balance', fontsize=10)
plt.xlabel('Value (in million)', fontsize=8)

# Boxplot for the highest credit limit
plt.subplot(1, 3, 3) # Third subplot position
sns.boxplot(data=df_idx, x='total_rev_hi_lim', color='green')
plt.title('Highest Credit Limit', fontsize=10)
plt.xlabel('Value (in million)', fontsize=8)

# Adjust the layout and display the plot
plt.tight_layout()
plt.show()

"""### **Check Invalid Value**"""

# Extract categorical features from the dataframe
categorical_features = df.select_dtypes(include=['object']).columns.tolist()

# Display the list of categorical columns
print("Identified Categorical Features:")
print(categorical_features)

# Display header for unique values output
print("\n=== Unique Values per Categorical Feature ===")

# Iterate through each categorical column and display its unique values
for column_name in categorical_features:
    # Extract unique values from the column
    unique_vals = df[column_name].unique()

# Display results in a clean format
    print(f"\nColumn: {column_name}")
    print(f"Unique values ({len(unique_vals)}):")
    print(unique_vals)

df.drop(['application_type'], axis=1, inplace=True)
df.shape

"""# **EDA - Insight**"""

df_eda = df.copy()
df_eda.sample(3)

"""Considering the number of variables is too large to analyze comprehensively, this research will focus on the `loan_amnt` feature as the main variable. This approach was chosen for:
1.   See the development of loan value from year to year
2.   Examine the correlation between loan amount and other predictive variables
3.   Simplify the complexity of the dataset without sacrificing key insights

# **Time Series Chart**
"""

df_datetime = df_eda.select_dtypes('datetime').columns.tolist()
print(df_datetime)

"""### **Date of Issue Loan Amount**"""

# Extract year and month components from the issue date
df_eda['year_issue_d'] = df_eda['date_issue_d'].dt.year  # Create year column
df_eda['month_issue_d'] = df_eda['date_issue_d'].dt.month  # Create month column

# Calculate monthly average loan amounts by year and month
df_viz1 = (df_eda
           .groupby(['year_issue_d', 'month_issue_d'])  # Group by year and month
           .agg({'loan_amnt': 'mean'})  # Calculate mean loan amount
           .reset_index()  # Convert groupby object to DataFrame
          )

# Rename columns for clearer interpretation
df_viz1.columns = ['Year', 'Month', 'Average_Loan_Amount']

# Display the resulting dataframe
df_viz1

# Set up the plot style and dimensions
plt.figure(figsize=(12, 8))
sns.set_style("white")

# Create point plot visualization
ax = sns.pointplot(
    x='Month',
    y="Average_Loan_Amount",
    data=df_viz1,
    hue="Year",
    palette="cubehelix",
    markers="o",
    linestyles="-"
)

# Format plot appearance
ax.set_facecolor('white')
plt.title(
    "Trend of Average Loan Amount Requests by Month and Year",
    fontsize=20,
    fontweight="bold",
    fontfamily='sans-serif',
    pad=20
)

# Axis labels formatting
plt.xlabel(
    'Month',
    fontfamily='sans-serif',
    fontsize=14,
    labelpad=10
)
plt.ylabel(
    'Average Loan Amount ($)',
    fontfamily='sans-serif',
    fontsize=14,
    labelpad=10
)

# Custom legend positioning (commented out as per original)
# ax.legend(title='Year', bbox_to_anchor=(1.05, 1), loc='upper left')

# Manual year labels (adjusted for better positioning)
year_labels = {
    2007: (11.2, 11419),
    2008: (11.2, 10000),
    2009: (11.2, 10917),
    2010: (11.2, 10500),
    2011: (11.2, 13985),
    2012: (11.2, 15805),
    2013: (11.2, 14719),
    2014: (11.2, 15275)
}

for year, (x_pos, y_pos) in year_labels.items():
    plt.text(
        x_pos, y_pos, str(year),
        fontsize=13,
        fontfamily='sans-serif',
        va='center'
    )

# Axis ticks formatting
plt.xticks(
    rotation=0,
    fontfamily='sans-serif',
    fontsize=12
)
plt.yticks(
    fontfamily='sans-serif',
    fontsize=12
)

# Final layout adjustments
plt.tight_layout()
plt.show()

"""# **Scatter Plot**

### **Loan Amount vs Funded Amount**
"""

# Create regression plot with custom styling
plt.figure(figsize=(10, 6))
ax = sns.regplot(
    x='loan_amnt',
    y='funded_amnt',
    data=df_eda,
    marker=".",  # Use small dot markers
    scatter_kws={
        'color': '#fc9f3c',  # Orange color for scatter points
        'alpha': 0.5,        # Semi-transparent points
        's': 60              # Marker size
    },
    line_kws={
        'color': "#0d5388",  # Dark blue for regression line
        'linewidth': 2.5     # Thicker regression line
    }
)

# Set white background
ax.set_facecolor('white')

# Customize axis labels and title
ax.set_xlabel(
    'Requested Loan Amount ($)',
    fontfamily='sans-serif',
    fontsize=12,
    labelpad=10
)
ax.set_ylabel(
    'Approved Funding Amount ($)',
    fontfamily='sans-serif',
    fontsize=12,
    labelpad=10
)
ax.set_title(
    'Relationship Between Requested and Approved Loan Amounts',
    fontsize=16,
    fontweight='bold',
    fontfamily='sans-serif',
    pad=20
)

# Format ticks
ax.tick_params(
    axis='both',
    which='major',
    labelsize=10,
    rotation=0
)

# Add grid for better readability
ax

"""# **Column Chart**

### **Count Bad Loan vs Good Loan**
"""

df_pp1 = df_eda.groupby(['proba_of_default']).agg({'loan_amnt':'count'}).reset_index()
df_pp1.columns = ['proba_of_default','total_user']
df_pp1['Ratio'] = round(df_pp1['total_user']/df_pp1['total_user'].sum()*100,0)
df_pp1

# Create custom color mapping - highlight bars with Ratio = 11% in orange
bar_colors = ['#fc9f3c' if ratio == 11 else 'grey' for ratio in df_pp1['Ratio']]

# Initialize plot with dark grid style
plt.figure(figsize=(10, 6))
sns.set_theme(style='darkgrid')

# Generate bar plot
ax = sns.barplot(
    data=df_pp1,
    x='proba_of_default',
    y='Ratio',
    palette=bar_colors,
    width=0.6  # Adjusted bar width
)

# Customize plot appearance
ax.set_facecolor('white')
plt.title(
    'User Distribution by Default Probability',
    size=18,
    weight='bold',
    fontfamily='sans-serif'
)

# Add value labels on bars
for container in ax.containers:
    ax.bar_label(
        container,
        fmt='%.1f%%',  # Format as percentage with 1 decimal
        padding=3,
        fontsize=10
    )

# Axis labels customization
ax.set_xlabel(
    'Default Probability',
    fontfamily='sans-serif',
    fontsize=12,
    labelpad=10
)
ax.set_ylabel(
    'Percentage of Users (%)',
    fontfamily='sans-serif',
    fontsize=12,
    labelpad=10
)

# Axis ticks formatting
ax.tick_params(
    axis='both',
    which='major',
    labelsize=10,
    rotation=0
)

# Set y-axis limit and display
plt.ylim(0, 100)
plt.tight_layout()
plt.show()

"""### **Count Objective Ratio**"""

# Calculate loan count by purpose category
purpose_distribution = (
    df_eda.groupby('purpose')['loan_amnt']  # Group by loan purpose
    .count()                                # Count loans per purpose
    .reset_index(name='total_users')        # Convert to DataFrame
)

# Calculate percentage distribution
purpose_distribution['percentage'] = round(
    purpose_distribution['total_users'] /
    purpose_distribution['total_users'].sum() * 100,
    1  # Round to 1 decimal place
)

# Sort by percentage (descending) and clean index
purpose_distribution = (
    purpose_distribution
    .sort_values('percentage', ascending=False)
    .reset_index(drop=True)  # Clean reset without keeping old index
)

# Display results
purpose_distribution

# Create color mapping - highlight bars with Ratio >= 12% in orange
bar_colors = ['#fc9f3c' if ratio >= 12 else 'lightgrey' for ratio in purpose_distribution['percentage']]

# Set up the visualization
plt.figure(figsize=(12, 8))
sns.set_theme(style='whitegrid')  # Cleaner white background with grid

# Create horizontal bar plot
ax = sns.barplot(
    data=purpose_distribution,
    x='percentage',
    y='purpose',
    palette=bar_colors,
    saturation=0.9,  # More vibrant colors
    width=0.8       # Better bar thickness
)

# Add value labels on bars
for container in ax.containers:
    ax.bar_label(
        container,
        fmt='%.1f%%',  # Format as percentage with 1 decimal
        padding=5,
        fontsize=10,
        fontweight='bold',
        color='#333333'
    )

# Customize plot appearance
ax.set_facecolor('white')
plt.title(
    'Distribution of Loan Purposes',
    fontsize=18,
    fontweight='bold',
    fontfamily='sans-serif',
    pad=20
)

# Axis labels
ax.set_xlabel(
    'Percentage of Total Loans (%)',
    fontfamily='sans-serif',
    fontsize=12,
    labelpad=10
)
ax.set_ylabel(
    'Loan Purpose',
    fontfamily='sans-serif',
    fontsize=12,
    labelpad=10
)

# Adjust ticks and limits
plt.xticks(fontfamily='sans-serif', fontsize=10)
plt.yticks(fontfamily='sans-serif', fontsize=10)
plt.xlim(0, 65)

# Add reference line at 12%
ax.axvline(x=12, color='#fc9f3c', linestyle='--', alpha=0.5)

plt.tight_layout()
plt.show()

"""**Personal loan vs Business loan**

*   Personal loan: debt_consolidation, credit_card, home_improvement, major_purchase, car, medical, moving, house, vacation, wedding, other
*   Business loan: small_business, educational, renewable_energy

"""

# Categorize loan purposes into business or personal loans
business_purposes = ['small_business', 'educational', 'renewable_energy']
df_eda['loan_category'] = np.where(
    df_eda['purpose'].isin(business_purposes),
    'business loan',
    'personal loan'
)

# Display the first 3 rows to verify
df_eda.head(3)

# Set font style (using default sans-serif for better compatibility)
font = {'family': 'sans-serif', 'size': 13, 'weight': 'normal'}

# Prepare data: select relevant columns and categorize default status
# Use loan_category from df_eda and proba_of_default from df_idx
loan_analysis = df_eda[['loan_category']].copy()
loan_analysis['proba_of_default'] = df['proba_of_default']

loan_analysis['default_status'] = np.where(
    loan_analysis['proba_of_default'] == 1,
    'Default',
    'Not Default'
)

# Create normalized crosstab
default_distribution = pd.crosstab(
    index=loan_analysis['loan_category'],  # Use loan_category
    columns=loan_analysis['default_status'],
    normalize='index'
).reindex(columns=['Not Default', 'Default'])  # Ensure consistent order

# Create visualization
plt.figure(figsize=(12, 7))
ax = default_distribution.plot(
    kind='bar',
    stacked=True,
    color=['#dddddd', '#fc9f3c'],  # Light grey and orange
    width=0.85,
    edgecolor='white'
)

# Reference line for average repayment rate
avg_repayment = loan_analysis['proba_of_default'].value_counts(normalize=True)[0]
plt.axhline(
    y=avg_repayment,
    color='#0d5388',
    linestyle='--',
    linewidth=2,
    alpha=0.7
)

# Customize plot appearance
ax.set_facecolor('white')
plt.title(
    'Loan Default Rates by Category',  # Updated title
    fontsize=16,
    fontweight='bold',
    pad=20
)
plt.xlabel('Loan Category', fontsize=12, labelpad=10) # Updated xlabel
plt.ylabel('Default Probability', fontsize=12, labelpad=10)

# Add annotation for reference line
plt.text(
    x=0.5,
    y=avg_repayment - 0.05,
    s=f'Overall Repayment Rate: {avg_repayment:.1%}',
    fontsize=11,
    color='#0d5388',
    ha='center'
)

# Format axes and legend
plt.ylim(0, 1.15)
plt.xticks(rotation=0, ha='center') # Adjusted rotation
plt.legend(
    title='Payment Status',
    bbox_to_anchor=(1, 1),
    frameon=False
)

plt.tight_layout()
plt.show()

"""###**Initial List Status Count Ratio**"""

# Prepare data: Select and transform relevant columns
loan_status_analysis = df_eda[['initial_list_status']].copy()
loan_status_analysis['proba_of_default'] = df['proba_of_default']
loan_status_analysis['default_status'] = np.where(
    loan_status_analysis['proba_of_default'] == 1,
    'Default',
    'Not Default'
)
loan_status_analysis['funding_status'] = np.where(
    loan_status_analysis['initial_list_status'] == 'f',
    'Fully Funded',
    'Partially Funded'
)

# Calculate proportional distribution
status_default_dist = pd.crosstab(
    index=loan_status_analysis['funding_status'],
    columns=loan_status_analysis['default_status'],
    normalize='index'
)[['Not Default', 'Default']]  # Ensure consistent column order

# Create visualization
plt.figure(figsize=(10, 6))
ax = status_default_dist.plot(
    kind='bar',
    stacked=True,
    color=['#e0e0e0', '#fc9f3c'],  # Light gray and orange
    width=0.6,
    edgecolor='white'
)

# Add reference line for overall repayment rate
overall_repayment_rate = df['proba_of_default'].value_counts(normalize=True)[0]
plt.axhline(
    y=overall_repayment_rate,
    color='#0d5388',
    linestyle='--',
    linewidth=2
)

# Customize plot appearance
ax.set_facecolor('white')
plt.title(
    'Default Rates by Initial Funding Status',
    fontsize=16,
    fontweight='bold',
    pad=20
)
plt.xlabel('Funding Status', fontsize=12)
plt.ylabel('Proportion', fontsize=12)

# Add annotation
plt.text(
    x=0.5,
    y=overall_repayment_rate - 0.1,
    s=f'Overall Repayment Rate: {overall_repayment_rate:.1%}',
    fontsize=11,
    ha='center',
    color='#0d5388'
)

# Format axes
plt.ylim(0, 1.15)
plt.xticks(rotation=0)
plt.legend(
    title='Loan Status',
    bbox_to_anchor=(1.25, 1),
    frameon=False
)

plt.tight_layout()
plt.show()

"""### **Interest Rate Ratio**"""

# Categorize interest rates into tiered groups
df_eda['int_rate_group'] = np.select(
    [
        df_eda['int_rate'] < 11,
        df_eda['int_rate'] < 15,
        df_eda['int_rate'] < 19,
        df_eda['int_rate'] < 23
    ],
    [
        '5%-10%',
        '10%-14%',
        '14%-18%',
        '18%-22%'
    ],
    default='22%-26%'  # For rates ‚â• 23%
)

# Prepare the data: select and transform relevant columns
interest_rate_analysis = df_eda[['int_rate_group']].copy()
interest_rate_analysis['proba_of_default'] = df['proba_of_default']
interest_rate_analysis['default_status'] = np.where(
    interest_rate_analysis['proba_of_default'] == 1,
    'Default',
    'Not Default'
)

# Calculate proportional distribution
rate_default_dist = pd.crosstab(
    index=interest_rate_analysis['int_rate_group'],
    columns=interest_rate_analysis['default_status'],
    normalize='index'
)[['Not Default', 'Default']]  # Ensure consistent column order

# Create proper ordering for interest rate groups
rate_order = ['5%-10%', '10%-14%', '14%-18%', '18%-22%', '22%-26%']
rate_default_dist = rate_default_dist.loc[rate_order]

# Create visualization
plt.figure(figsize=(12, 7))
ax = rate_default_dist.plot(
    kind='bar',
    stacked=True,
    color=['#e0e0e0', '#fc9f3c'],  # Light gray and orange
    width=0.7,
    edgecolor='white'
)

# Add reference line for overall repayment rate
overall_repayment = df['proba_of_default'].value_counts(normalize=True)[0]
plt.axhline(
    y=overall_repayment,
    color='#0d5388',
    linestyle='--',
    linewidth=2
)

# Customize plot appearance
ax.set_facecolor('white')
plt.title(
    'Default Rates by Interest Rate Tier',
    fontsize=18,
    fontweight='bold',
    pad=20
)
plt.xlabel('Interest Rate Group', fontsize=14)
plt.ylabel('Default Probability', fontsize=14)

# Add annotation
plt.text(
    x=2,  # Middle position
    y=overall_repayment + 0.05,
    s=f'Overall Repayment Rate: {overall_repayment:.1%}',
    fontsize=12,
    ha='center',
    color='#0d5388',
    fontweight='bold'
)

# Format axes
plt.ylim(0, 1.15)
plt.xticks(rotation=0)
plt.legend(
    title='Loan Status',
    bbox_to_anchor=(1.15, 1),
    frameon=False
)

plt.tight_layout()
plt.show()

"""# **EDA - Redundant Features**

#### **Drop EDA - Insight**

#### **Drop High Correlation**
"""

#TIdak usah di running

#columns_to_drop = [
    #'id','member_id','unnamed:0', 'policy_code','annual_inc_joint','dti_joint',
    #'verification_status_joint', 'open_acc_6m','open_il_6m','open_il_12m','open_il_24m','mths_since_rcnt_il',
    #'total_bal_il','il_util','open_rv_12m','open_rv_24m','open_rv_24m','max_bal_bc','all_util',
    #'inq_fi','total_cu_tl','inq_last_12m'
    #]

#TIdak usah di running
 # def safe_drop_columns(df, cols_to_drop):
  #  """Fungsi aman untuk menghapus kolom"""
   # existing_cols = [col for col in cols_to_drop
             #       if col in df.columns]

  #  if len(existing_cols) < len(cols_to_drop):
     #   missing = set(cols_to_drop) - set(existing_cols)
    #    print(f"Warning: {len(missing)} kolom tidak ditemukan: {missing}")

   # return df.drop(columns=existing_cols)

# Gunakan fungsi
#df_clean = safe_drop_columns(df, columns_to_drop)
#print(f"Shape sebelum: {df.shape}, sesudah: {df_clean.shape}")

# Select only numeric columns for correlation calculation
numeric_df = df.select_dtypes(include=np.number)

# Set up the figure dimensions for better readability
plt.figure(figsize=(25, 18))

# Create a comprehensive correlation heatmap
correlation_matrix = numeric_df.corr()
sns.heatmap(
    correlation_matrix,
    cmap='magma',        # Color scheme optimized for visibility
    annot=True,          # Show correlation values
    fmt='.2f',           # Format to 2 decimal places
    annot_kws={'size': 10},  # Adjust annotation font size
    linewidths=0.5,      # Add subtle grid lines
    vmin=-1, vmax=1      # Fix color scale from -1 to 1
)

# Add title and display
plt.title(
    'Feature Correlation Matrix (Numeric Features Only)', # Updated title
    fontsize=20,
    pad=20
)
plt.tight_layout()
plt.show()

# Show dataset dimensions after processing
print("\nDataset Dimensions (after dropping temporal and redundant features):") # Updated print statement
display(df.shape)

"""Based on the correlation heatmap results, many overlapping or redundant features were found. To avoid the problem of multicollinearity, one of the feature pairs that has a correlation of more than 0.7 will be removed. The threshold of 0.7 was chosen because it provides a good balance between removing features that are too similar and retaining features that are still related but carry different information. Correlations below this number are generally considered weak or insignificant, while correlations above 0.7 are considered quite strong and tend to retain similar information. Therefore, a correlation threshold of 0.7 is considered appropriate to consider both aspects.


"""

import pandas as pd
import numpy as np

# 1. Filter only numeric columns
numeric_df = df.select_dtypes(include=['int64', 'float64'])

# 2. Calculate absolute correlation matrix
abs_corr_matrix = numeric_df.corr().abs()

# 3. Create upper triangular mask (no diagonal)
mask = np.triu(np.ones_like(abs_corr_matrix, dtype=bool), k=1)
upper_triangle = abs_corr_matrix.where(mask)

# 4. Identify columns with correlation >0.7
high_corr_cols = [
    col for col in upper_triangle.columns
    if any(upper_triangle[col] > 0.7)
]

# 5. Show results
print("Fitur dengan korelasi tinggi (>0.7):")
high_corr_cols

# 1. Only drop columns that actually exist in the dataframe
cols_to_drop = [col for col in high_corr_cols if col in df.columns]

# 2. Drop columns by checking
if cols_to_drop:  # Jika ada kolom yang perlu di-drop
    df.drop(columns=cols_to_drop, inplace=True)
    print(f"Dropped {len(cols_to_drop)} highly correlated columns")
else:
    print("No columns to drop - all specified columns not found in dataframe")

# 3. Verify final dimensions
print("\nFinal dataset dimensions:", df.shape)

# Take only numeric columns
numeric_df = df.select_dtypes(include=['int64', 'float64'])

# Remove columns where all values are equal (zero variance)
numeric_df = numeric_df.loc[:, numeric_df.nunique() > 1]

# Remove columns where all values are NaN after correlation
corr_matrix = numeric_df.corr()
corr_matrix = corr_matrix.dropna(axis=1, how='all').dropna(axis=0, how='all')

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# Select numeric columns
numeric_df = df.select_dtypes(include=['int64', 'float64'])

# Remove columns with zero variance
numeric_df = numeric_df.loc[:, numeric_df.nunique() > 1]

# 4. Calculate correlation matrix
corr_matrix = numeric_df.corr()

# Visualize the heat map
plt.figure(figsize = (25, 18))
sns.heatmap(
 corr_matrix,
 cmap='magma',
 annot=True,
 fmt='.2f',
 annot_kws={'size': 10},
 linewidths=0.5,
 vmin=-1, vmax=1
)
plt.title('Feature Correlation Matrix (Cleaned)', fontsize=20, pad=20)
plt.tight_layout()
plt.show()

"""## **Drop High Cardinality**"""

# Get categorical columns and calculate uniqueness metrics
(
    df.select_dtypes(include=['object'])
    .nunique()
    .to_frame('Unique Values')
    .sort_values('Unique Values', ascending=False)
    .assign(Percentage=lambda x: x['Unique Values']/len(df)*100)
)

"""High cardinality refers to columns in the data that have a large number of unique values or different categories. Based on the features, the columns ` emp_title, title, issue_d, addr_state` will be removed. This is done to help improve the efficiency of the algorithm and hopefully reduce the possibility of overfitting."""

df = df.drop(columns=['emp_title', 'title', 'issue_d', 'addr_state'
], errors='ignore')
df.shape

"""### **Drop Imbalance Class**"""

categorical_columns = df.select_dtypes(include=['object']).columns.tolist()
print(categorical_columns)

print("Evaluating Class Imbalance Across Categorical Variables\n")

for categorical_feature in categorical_columns:
    # Calculate percentage distribution of each category
    percentage_dist = df[categorical_feature].value_counts(normalize=True) * 100
    print(f"Category: {categorical_feature}")
    print(f"Class Distribution (%):\n{percentage_dist}\n{'='*30}")

df.drop('pymnt_plan', axis=1, inplace=True)

"""# **Feature Extraction**"""

purpose_map = {
    'small_business': 'business loan',
    'educational': 'business loan',
    'renewable_energy': 'business loan'
}

df['purpose'] = df['purpose'].map(lambda x: purpose_map.get(x, 'personal loan'))
df.head(3)

ownership_map = {'ANY': 'OTHER', 'OTHER': 'OTHER'}
df['home_ownership'] = df['home_ownership'].map(lambda x: ownership_map.get(x, x))
df['home_ownership'].value_counts()

"""# **Handling Missing Values**

### **Drop Columns with more than 20% Missing Values**

Removing columns with over 20% missing values is standard practice in data preprocessing as these variables typically contain insufficient meaningful information for analysis or modeling. High percentages of missing data can introduce significant bias or noise into analytical processes."
"""

# Calculate and sort missing value percentages using method chaining
(
    df.isna()
    .mean()
    .mul(100)
    .sort_values(ascending=False)
)

df.drop(['mths_since_last_record','mths_since_last_delinq',
         'date_next_pymnt_d','emp_length_int'] , axis=1, inplace=True)

"""# **Fill Missing Values**

Features that contain missing values will be imputed using the median value. The selection of this method is based on the consideration that in the next pre-processing stage, random undersampling will be carried out to reach around 90 thousand observations. The use of extreme values such as **0 or -1** (which are outside the actual data distribution) has the potential to disrupt the optimal data distribution.
"""

print(df['tot_cur_bal'].describe())
print(f"Missing Values Count: {df['tot_cur_bal'].isna().sum()} " )

df['tot_cur_bal'].fillna(df['tot_cur_bal'].median(), inplace=True)

print(df['tot_coll_amt'].describe())
print(f"Missing Values Count: {df['tot_coll_amt'].isna().sum()} " )

df['tot_coll_amt'].fillna(df['tot_coll_amt'].median(), inplace=True)

# Show summary statistics for revolving utilization
display(df['revol_util'].describe())

# Print count of missing values
print('Missing Values Count:', df['revol_util'].isna().sum())

df['revol_util'].fillna(df['revol_util'].median(), inplace=True)

# Analyze credit history duration
credit_history_stats = df['months_since_earliest_cr_line'].describe()
display(credit_history_stats)

# Check data completeness
missing_values = df['months_since_earliest_cr_line'].isna().sum()
print(f'Total Missing Credit History Records: {missing_values}')

df['months_since_earliest_cr_line'].fillna(df['months_since_earliest_cr_line'].median(), inplace=True)

# Analyze collection records (non-medical, past 12 months)
collection_stats = df['collections_12_mths_ex_med'].describe()
display(collection_stats)

# Check data completeness
null_count = df['collections_12_mths_ex_med'].isna().sum()
print(f'Total Missing Collection Records: {null_count}')

default_value = 0
df['collections_12_mths_ex_med'].fillna(default_value, inplace=True)

# Analyze collection records (non-medical, past 12 months)
collection_stats = df['total_acc'].describe()
display(collection_stats)

# Check data completeness
null_count = df['total_acc'].isna().sum()
print(f'Total Missing Collection Records: {null_count}')

default_value = 0
df['total_acc'].fillna(default_value, inplace=True)

# Analyze collection records (non-medical, past 12 months)
collection_stats = df['acc_now_delinq'].describe()
display(collection_stats)

# Check data completeness
null_count = df['acc_now_delinq'].isna().sum()
print(f'Total Missing Collection Records: {null_count}')

default_value = 0
df['acc_now_delinq'].fillna(default_value, inplace=True)

# Analyze collection records (non-medical, past 12 months)
collection_stats = df['pub_rec'].describe()
display(collection_stats)

# Check data completeness
null_count = df['pub_rec'].isna().sum()
print(f'Total Missing Collection Records: {null_count}')

default_value = 0
df['pub_rec'].fillna(default_value, inplace=True)

# Analyze collection records (non-medical, past 12 months)
collection_stats = df['open_acc'].describe()
display(collection_stats)

# Check data completeness
null_count = df['open_acc'].isna().sum()
print(f'Total Missing Collection Records: {null_count}')

default_value = 0
df['open_acc'].fillna(default_value, inplace=True)

# Analyze collection records (non-medical, past 12 months)
collection_stats = df['inq_last_6mths'].describe()
display(collection_stats)

# Check data completeness
null_count = df['inq_last_6mths'].isna().sum()
print(f'Total Missing Collection Records: {null_count}')

default_value = 0
df['inq_last_6mths'].fillna(default_value, inplace=True)

# Analyze collection records (non-medical, past 12 months)
collection_stats = df['delinq_2yrs'].describe()
display(collection_stats)

# Check data completeness
null_count = df['delinq_2yrs'].isna().sum()
print(f'Total Missing Collection Records: {null_count}')

default_value = 0
df['delinq_2yrs'].fillna(default_value, inplace=True)

# Analyze collection records (non-medical, past 12 months)
collection_stats = df['annual_inc'].describe()
display(collection_stats)

# Check data completeness
null_count = df['annual_inc'].isna().sum()
print(f'Total Missing Collection Records: {null_count}')

default_value = 0
df['annual_inc'].fillna(default_value, inplace=True)

display(df.head())
print(df.shape)

"""Since the term and term_int columns are the same, it is necessary to delete 1 column. Here the column to be deleted is the term column"""

df = df.drop('term', axis=1)
display(df.head())
print(df.shape)

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# Select numeric columns
numeric_df = df.select_dtypes(include=['int64', 'float64'])

# Remove columns with zero variance
numeric_df = numeric_df.loc[:, numeric_df.nunique() > 1]

# 4. Calculate correlation matrix
corr_matrix = numeric_df.corr()

# Visualize the heat map
plt.figure(figsize = (25, 18))
sns.heatmap(
 corr_matrix,
 cmap='magma',
 annot=True,
 fmt='.2f',
 annot_kws={'size': 10},
 linewidths=0.5,
 vmin=-1, vmax=1
)
plt.title('', fontsize=20, pad=20)
plt.tight_layout()
plt.show()

"""# **Drop Feature**"""

df.info()

print(df.columns)

df.drop([
    'months_since_earliest_cr_line',
    'open_acc', 'revol_bal', 'delinq_2yrs',
    'collections_12_mths_ex_med', 'inq_last_6mths',
    'tot_coll_amt', 'acc_now_delinq',
    'initial_list_status', 'verification_status', 'grade'
], axis=1, inplace=True)

df.info()

"""### **Split Dataset**"""

from sklearn.model_selection import train_test_split

assert 'proba_of_default' in df.columns, "Kolom target tidak ditemukan"

# Split dataset
X = df.drop(columns='proba_of_default')
y = df['proba_of_default']

X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.3,
    random_state=69,
    stratify=y
)

print(f"Data Dimension:")
print(f"X_train: {X_train.shape}, X_test: {X_test.shape}")

y_test.value_counts()

"""# **Encoding & Standarization**"""

print("Memproses tipe fitur...")

categorical_features = X.select_dtypes(include=['object']).columns
print(f"Found {len(categorical_features)} categorical features")

numerical_features = X.select_dtypes(include=['int32', 'int64', 'float64']).columns
print(f"Found {len(numerical_features)} numerical features")

"""# **OneHot Encoding and Standard Gauge**"""

from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler

# Konfigurasi yang kompatibel
OHE_CONFIG = {
    'handle_unknown': 'ignore',
    'drop': 'first'
}

SCALER_CONFIG = {
    'with_mean': True,
    'with_std': True
}

preprocessor = ColumnTransformer([
    ('numerical', Pipeline([
        ('scaler', StandardScaler(**SCALER_CONFIG))
    ]), numerical_features),

    ('categorical', Pipeline([
        ('onehot', OneHotEncoder(**OHE_CONFIG))
    ]), categorical_features)
])

# fit and transform the train data
X_train_processed = preprocessor.fit_transform(X_train)

# transform the test data
X_test_processed = preprocessor.transform(X_test)

print(preprocessor.named_transformers_.keys())

# Import library
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder

#1. Define the pipeline first
numerical_pipeline = Pipeline([
('scaler', StandardScaler())
])

categorical_pipeline = Pipeline([
('onehot', OneHotEncoder(handle_unknown='ignore', drop='first'))
])

# 2. Just create a ColumnTransformer
preprocessor = ColumnTransformer([
('num_processor', numerical_pipeline, numerical_features),
('cat_processor', categorical_pipeline, categorical_features)
])

# 3. Now it can be used
preprocessor.fit(X_train)

numerical_features = X_train.select_dtypes(include=['int64', 'float64']).columns
categorical_features = X_train.select_dtypes(include=['object']).columns

from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder
import pandas as pd
import numpy as np

# 1. Define numerical_features and categorical_features
numerical_features = X_train.select_dtypes(include=['int64', 'float64']).columns
categorical_features = X_train.select_dtypes(include=['object']).columns

# 2. Create a pipeline for each feature type
numerical_pipeline = Pipeline([
    ('scaler', StandardScaler())
])

categorical_pipeline = Pipeline([
    ('onehot_encoder', OneHotEncoder(handle_unknown='ignore', drop='first'))
])

# 3. Create a ColumnTransformer
preprocessor = ColumnTransformer([
    ('numerical', numerical_pipeline, numerical_features),
    ('categorical', categorical_pipeline, categorical_features)
])

# 4. Fit and transform data
X_train_processed = preprocessor.fit_transform(X_train)
X_test_processed = preprocessor.transform(X_test)

# 5. Get feature names after preprocessing
try:
    # Check the available step names
    print("Available transformers:", preprocessor.named_transformers_.keys())

    # Get the encoder from the categorical pipeline
    categorical_pipe = preprocessor.named_transformers_['categorical']
    print("Steps in categorical pipeline:", categorical_pipe.named_steps.keys())

    # Extract the name of the encoded feature
    ohe_encoder = categorical_pipe.named_steps['onehot_encoder']
    ohe_feature_names = ohe_encoder.get_feature_names_out(categorical_features)

    # Combine with numeric features
    feature_names = list(numerical_features) + list(ohe_feature_names)

    # 6. Create a DataFrame with column names
    X_train_df = pd.DataFrame(X_train_processed, columns=feature_names, index=X_train.index)
    X_test_df = pd.DataFrame(X_test_processed, columns=feature_names, index=X_test.index)

    print("\nPreprocessing result DataFrame:")
    print("Training shape:", X_train_df.shape)
    print("Testing shape:", X_test_df.shape)
    print("\n5 sample training data:")
    display(X_train_df.head())

except Exception as e:
    print("\nError:", str(e))
    print("\nMake sure:")
    print("- Step names in Pipeline and ColumnTransformer are consistent")
    print("- The categorical_features column matches the input data")

pd.DataFrame(X_train_processed, columns=feature_names).head(3)

pd.DataFrame(X_train_processed, columns=feature_names, index=X_train.index).describe()

pd.DataFrame(X_test_processed, columns=feature_names, index=X_test.index).head(3)

pd.DataFrame(X_test_processed, columns=feature_names, index=X_test.index).describe()

print(f"Train: {X_train_processed.shape[0]} row, {X_train_processed.shape[1]} column")
print(f"Test : {X_test_processed.shape[0]} row, {X_test_processed.shape[1]} column")

"""# **Undersampling**

I use RandomUnderSampler because my laptop specifications do not support running other under sampling methods such as Tomek Links or ENN, or oversampling techniques, considering that this credit risk dataset has a very large amount of data.
"""

print(y_train.value_counts())

# Display the class distribution after undersampling
from imblearn.under_sampling import RandomUnderSampler

# Initialize RandomUnderSampler with random_state for reproducibility
rus = RandomUnderSampler(random_state=69)

# Undersample the training data
X_undersampled, y_undersampled = rus.fit_resample(X_train_processed, y_train)

# Calculate and print the number of samples per class after undersampling
print(y_undersampled.value_counts())

X_undersampled_df = pd.DataFrame(X_undersampled, columns=feature_names)
X_undersampled_df.info()

"""# **Modelling**

### **Function**
"""

from sklearn.metrics import accuracy_score, recall_score, roc_auc_score, precision_score, f1_score, confusion_matrix

def evaluate_model(model):
    # Get predictions for test data and training data
    test_pred = model.predict(X_test_processed)
    train_pred = model.predict(X_undersampled)

    # Get prediction probabilities
    test_proba = model.predict_proba(X_test_processed)
    train_proba = model.predict_proba(X_undersampled)

    # Evaluate model performance with undersampling
    print('\n=== MODEL EVALUATION WITH UNDERSAMPLING ===')
    print('\nAUC Score Comparison:')
    print(f'AUC Score Training: {roc_auc_score(y_undersampled, train_proba[:,1]):.6f}')
    print(f'AUC Score Testing: {roc_auc_score(y_test, test_proba[:,1]):.6f}\n')

    # Display other evaluation metrics
    print('Other Evaluation Metrics:')
    print(f'Testing Accuracy : {accuracy_score(y_test, test_pred):.4f}')
    print(f'Precision Testing : {precision_score(y_test, test_pred):.4f}')
    print(f'Recall Testing : {recall_score(y_test, test_pred):.4f}')
    print(f'F1-Score Testing: {f1_score(y_test, test_pred):.4f}\n')

    # Display confusion matrix
    print('=== CONFUSION MATRIX ===')
    print(confusion_matrix(y_test, test_pred))

"""### **Logistic Regression**"""

from sklearn.linear_model import LogisticRegression

# Initialize and train logistic regression model on undersampled data
logreg_model = LogisticRegression()
logreg_model.fit(X_undersampled, y_undersampled)

# Evaluate the trained logistic regression model's performance
evaluate_model(logreg_model)

"""### **Decision Tree**"""

from sklearn.tree import DecisionTreeClassifier

# Create a Decision Tree model with random states for reproducibility
decision_tree = DecisionTreeClassifier(random_state=69)

# Train the model using undersampled data
decision_tree.fit(X_undersampled, y_undersampled)

evaluate_model(decision_tree)

"""### **Random Forest**"""

from sklearn.ensemble import RandomForestClassifier

rf_model = RandomForestClassifier(random_state=69)

rf_model.fit(X_undersampled, y_undersampled)

evaluate_model(rf_model)

"""### **Adaptive Boosting (Adaboost)**"""

from sklearn.ensemble import AdaBoostClassifier
adaboost_model = AdaBoostClassifier(random_state=69)
adaboost_model.fit(X_undersampled, y_undersampled)

evaluate_model(adaboost_model)

"""### **XGBoost**"""

from xgboost import XGBClassifier  # Import library

xgb_model = XGBClassifier(random_state=69)
xgb_model.fit(X_undersampled, y_undersampled)

evaluate_model(xgb_model)

"""### **Light Gradient Boosting Machine (LGBM)**



"""

from lightgbm import LGBMClassifier

lgbm_model = LGBMClassifier(random_state=69)
lgbm_model.fit(X_undersampled, y_undersampled)

evaluate_model(lgbm_model)

"""Based on the results of the model that has been used, it can be summarized:
1. The model produces excellent `precision` values (‚â• 0.9). All positive predictions (**positive predictions**) made by the model are completely valid (**true positives**). There are no false positives.

2. The `accuracy, precision, recall, and F1-score` values all reached 1.0000, meaning:
The model is perfect in predicting the testing data-no misclassification (all predictions are correct, both for positive and negative classes).

# **Hyperparameter Tuning**
"""

def evaluate_tuned_model(model):

    # Generate predictions and probabilities
    test_predictions = model.predict(X_test_processed)
    train_predictions = model.predict(X_undersampled)
    test_probabilities = model.predict_proba(X_test_processed)
    train_probabilities = model.predict_proba(X_undersampled)

    # Calculate evaluation metrics
    train_auc = roc_auc_score(y_undersampled, train_probabilities[:, 1])
    test_auc = roc_auc_score(y_test, test_probabilities[:, 1])

    test_accuracy = accuracy_score(y_test, test_predictions)
    test_precision = precision_score(y_test, test_predictions)
    test_recall = recall_score(y_test, test_predictions)
    test_f1 = f1_score(y_test, test_predictions)

    # Display evaluation results
    print('\n=== MODEL EVALUATION WITH UNDERSAMPLING ===')
    print('\nAUC Scores:')
    print(f'Training AUC: {round(train_auc, 6)}')
    print(f'Test AUC: {round(test_auc, 6)}\n')

    print('Classification Metrics:')
    print(f'Test Accuracy: {round(test_accuracy, 4)}')
    print(f'Test Precision: {round(test_precision, 4)}')
    print(f'Test Recall: {round(test_recall, 4)}')
    print(f'Test F1-Score: {round(test_f1, 4)}\n')

    print('=== CONFUSION MATRIX ===')
    print(confusion_matrix(y_test, test_predictions))

from sklearn.model_selection import RandomizedSearchCV

"""### **Logistic Regression**"""

# Initialize Logistic Regression model
logistic_model = LogisticRegression()

# Define hyperparameter search space
logistic_hyperparams = {
    "penalty": ['l1', 'l2'],  # Regularization type (L1/Lasso or L2/Ridge)
    "C": [float(x) for x in np.linspace(0.001, 10, 100)]  # Inverse of regularization strength (100 values between 0.001 and 10)
}

# Set up randomized search with cross-validation
tuned_logistic = RandomizedSearchCV(
    estimator=logistic_model,
    param_distributions=logistic_hyperparams,
    cv=5,  # 5-fold cross-validation
    random_state=69,  # For reproducibility
    n_jobs=-1,  # Use all available CPU cores
    scoring='recall',  # Optimize for recall metric
    n_iter=50  # Number of parameter combinations to try (added for better practice)
)

# Perform hyperparameter tuning on undersampled data
tuned_logistic.fit(X_undersampled, y_undersampled)

print(f'Optimal parameters found: {tuned_logistic.best_params_}')

evaluate_tuned_model(tuned_logistic)

"""### **Decision Tree**"""

# Initialize the basic Decision Tree model
decision_tree = DecisionTreeClassifier()

# Setting up hyperparameter distribution for tuning
tree_hyperparams = {
 "criterion": ['gini', 'entropy'], # Split quality measurement method
 "max_depth": [5, 14, 24], # Maximum depth of the tree
 "min_samples_split": [2, 10, 100], # Minimum number of samples for split node
 "min_samples_leaf": [2, 10, 100], # Minimum number of samples at leaf nodes
 "max_features": ['auto', 'sqrt'] # Number of features considered for split
}

# Setup Randomized Search with Cross-Validation
optimized_tree = RandomizedSearchCV(
 estimator=decision_tree,
 param_distributions=tree_hyperparams,
 cv=5, # 5-fold cross validation
 random_state=69, # For reproducibility
 n_jobs=-1, # Using all CPU cores
 scoring='recall', # Focus optimization on recall
 n_iter=50 # Number of parameter combinations to try
)

# Tuning the undersampled data
optimized_tree.fit(X_undersampled, y_undersampled)

print(f'Optimal Decision Tree parameters: {optimized_tree.best_params_}')

evaluate_tuned_model(optimized_tree)

"""### **Random Forest**"""

# Initialize base Random Forest model
random_forest = RandomForestClassifier()

# Define hyperparameter search space
rf_hyperparams = {
    "n_estimators": [5, 10, 20, 30],       # Number of trees in the forest
    "criterion": ['gini', 'entropy'],      # Splitting criterion
    "max_depth": [5, 10, 15],              # Maximum tree depth
    "min_samples_split": [5, 10, 20],      # Minimum samples required to split a node
    "min_samples_leaf": [5, 10, 20],       # Minimum samples required at a leaf node
    "max_features": ['auto', 'sqrt']       # Number of features to consider for splits
}

# Configure randomized search with cross-validation
optimized_rf = RandomizedSearchCV(
    estimator=random_forest,
    param_distributions=rf_hyperparams,
    cv=5,                   # 5-fold cross-validation
    random_state=69,        # For reproducibility
    n_jobs=-1,              # Utilize all available CPU cores
    scoring='recall',       # Optimize for recall metric
    n_iter=50               # Number of parameter combinations to sample
)

# Perform hyperparameter tuning on undersampled data
optimized_rf.fit(X_undersampled, y_undersampled)

print(f'Random Forest Optimal Hyperparameter Combination: {optimized_rf.best_params_}')

evaluate_tuned_model(optimized_rf)

"""### **Adaboost**"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# from sklearn.ensemble import AdaBoostClassifier
# from sklearn.model_selection import RandomizedSearchCV
# 
# adaboost_model = AdaBoostClassifier()
# 
# adaboost_params = {
#     "n_estimators": [75, 80],
#     "learning_rate": [0.5, 0.7],
#     "algorithm": ['SAMME']  # Parameter yang benar
# }
# 
# tuned_adaboost_model = RandomizedSearchCV(
#     estimator=adaboost_model,
#     param_distributions=adaboost_params,
#     cv=5,
#     random_state=69,
#     n_jobs=-1,
#     scoring='recall',
#     n_iter=10
# )
# 
# tuned_adaboost_model.fit(X_undersampled, y_undersampled)

print(f'Optimal parameters found: {tuned_adaboost_model.best_params_}')

evaluate_tuned_model(tuned_adaboost_model)

"""### **XGBoost**"""

from xgboost import XGBClassifier
from sklearn.model_selection import RandomizedSearchCV
import numpy as np

# 1. Pertama, definisikan parameter space untuk random search
xgb_params = {
    'max_depth': np.arange(3, 10),
    'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3],
    'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],
    'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],
    'gamma': [0, 0.1, 0.2, 0.3, 0.4],
    'min_child_weight': [1, 3, 5]
}

# 2. Konfigurasi model dasar dengan early stopping
xgb_model = XGBClassifier(
    n_estimators=190,
    early_stopping_rounds=10,
    eval_metric='auc',
    random_state=42
)

# 3. Setup RandomizedSearchCV
optimized_xgb = tuned_xgboost = RandomizedSearchCV( # Assign the result to both variables
    estimator=xgb_model,
    param_distributions=xgb_params,  # Sekarang xgb_params sudah terdefinisi
    cv=5,
    random_state=69,
    n_jobs=-1,
    scoring='recall',
    n_iter=20
)

# 4. Fitting model
optimized_xgb.fit(
    X_undersampled,
    y_undersampled,
    eval_set=[(X_test_processed, y_test)],
    verbose=1
)

print(f'Optimal parameters found: {optimized_xgb.best_params_}')

evaluate_tuned_model(optimized_xgb)

"""### **Light Gradient Boosting Machine (LGBM)**"""

from lightgbm import early_stopping
from sklearn.model_selection import RandomizedSearchCV
from lightgbm import LGBMClassifier

lgbm_model = LGBMClassifier()

lgbm_params = {
    'max_depth': [4, 5],
    'min_child_samples': [20, 25],
    'learning_rate': [0.8, 0.9],
    'num_leaves': [15, 17],
    'subsample': [0.3, 0.4],
    'colsample_bytree': [0.8, 0.85],
    'reg_alpha': [0.15, 0.19],
    'reg_lambda': [0.1, 0.8]
}

optimized_lgbm = RandomizedSearchCV(
    estimator=lgbm_model,
    param_distributions=lgbm_params,
    cv=5,
    random_state=69,
    n_jobs=-1,
    scoring='recall',
    n_iter=50,
    verbose=1
)

# Fit dengan early stopping melalui callbacks
optimized_lgbm.fit(
    X_undersampled,
    y_undersampled,
    eval_set=[(X_test_processed, y_test)],
    callbacks=[early_stopping(stopping_rounds=10)],
)

print(f'Optimal parameters found: {optimized_lgbm.best_params_}')

evaluate_tuned_model(optimized_lgbm)

"""# **Models Evaluation**"""

# Generate confusion matrices for all tuned models
logreg_conf_matrix = confusion_matrix(y_test, tuned_logistic.predict(X_test_processed))
dtree_conf_matrix = confusion_matrix(y_test, optimized_tree.predict(X_test_processed))
rf_conf_matrix = confusion_matrix(y_test, optimized_rf.predict(X_test_processed))
adaboost_conf_matrix = confusion_matrix(y_test, tuned_adaboost_model.predict(X_test_processed))
xgb_conf_matrix = confusion_matrix(y_test, optimized_xgb.predict(X_test_processed))
lgbm_conf_matrix = confusion_matrix(y_test, optimized_lgbm.predict(X_test_processed))

def create_metrics_dataframe(model_name: str, confusion_matrix: np.ndarray) -> pd.DataFrame:
    """
    Convert a confusion matrix into a DataFrame of performance metrics.

    Args:
        model_name (str): Name of the model for column title
        confusion_matrix (np.ndarray): 2x2 confusion matrix

    Returns:
        pd.DataFrame: DataFrame containing calculated metrics
    """
    # Extract confusion matrix components
    true_neg, false_pos, false_neg, true_pos = confusion_matrix.ravel()

    # Calculate metrics
    accuracy = round((true_pos + true_neg) / (true_neg + true_pos + false_neg + false_pos), 4)
    recall = round(true_pos / (false_neg + true_pos), 4)  # Also known as sensitivity
    precision = round(true_pos / (true_pos + false_pos), 4)
    f1_score = round(2 * (1/((1/precision) + (1/recall))), 4)

    # Create formatted DataFrame
    metrics_df = pd.DataFrame(
        data=[f1_score, accuracy, recall, precision],
        columns=[model_name],
        index=["F1 Score", "Accuracy", "Recall (Sensitivity)", "Precision"]
    )

    return metrics_df

# Create metrics DataFrames for all models
logreg_metrics = create_metrics_dataframe('Optimized Logistic Regression', logreg_conf_matrix)
dtree_metrics = create_metrics_dataframe('Optimized Decision Tree', dtree_conf_matrix)
rf_metrics = create_metrics_dataframe('Optimized Random Forest', rf_conf_matrix)
adaboost_metrics = create_metrics_dataframe('Optimized AdaBoost', adaboost_conf_matrix)
xgb_metrics = create_metrics_dataframe('Optimized XGBoost', xgb_conf_matrix)
lgbm_metrics = create_metrics_dataframe('Optimized LightGBM', lgbm_conf_matrix)

def create_auc_dataframe(model, model_name: str, X_train, y_train, X_test, y_test) -> pd.DataFrame:
    """
    Create a DataFrame of AUC scores for train and test sets

    Args:
        model: Trained model with predict_proba() method
        model_name (str): Name of the model for display
        X_train: Training features
        y_train: Training labels
        X_test: Test features
        y_test: Test labels

    Returns:
        pd.DataFrame: DataFrame with train and test AUC scores
    """
    train_auc = round(roc_auc_score(y_train, model.predict_proba(X_train)[:, 1]), 5)
    test_auc = round(roc_auc_score(y_test, model.predict_proba(X_test)[:, 1]), 5)

    return pd.DataFrame(
        data=[train_auc, test_auc],
        columns=[model_name],
        index=['AUC-Proba Train', 'AUC-Proba Test']
    )

# Create AUC DataFrames for all models
auc_logreg = create_auc_dataframe(tuned_logistic, 'Optimized Logistic Regression',
                                 X_undersampled, y_undersampled,
                                 X_test_processed, y_test)

auc_dtree = create_auc_dataframe(optimized_tree, 'Optimized Decision Tree',
                               X_undersampled, y_undersampled,
                               X_test_processed, y_test)

auc_rf = create_auc_dataframe(optimized_rf, 'Optimized Random Forest',
                            X_undersampled, y_undersampled,
                            X_test_processed, y_test)

auc_adaboost = create_auc_dataframe(tuned_adaboost_model, 'Optimized AdaBoost',
                                  X_undersampled, y_undersampled,
                                  X_test_processed, y_test)

auc_xgb = create_auc_dataframe(optimized_xgb, 'Optimized XGBoost',
                             X_undersampled, y_undersampled,
                             X_test_processed, y_test)

auc_lgbm = create_auc_dataframe( optimized_lgbm, 'Optimized LightGBM',
                              X_undersampled, y_undersampled,
                              X_test_processed, y_test)

# Combine AUC metrics with classification metrics for each model
final_logreg_metrics = pd.concat([auc_logreg, logreg_metrics])
final_dtree_metrics = pd.concat([auc_dtree, dtree_metrics])
final_rf_metrics = pd.concat([auc_rf, rf_metrics])
final_adaboost_metrics = pd.concat([auc_adaboost, adaboost_metrics])
final_xgb_metrics = pd.concat([auc_xgb, xgb_metrics])
final_lgbm_metrics = pd.concat([auc_lgbm, lgbm_metrics])

# Define the desired metric order for consistent presentation
METRIC_ORDER = [
    "AUC-Proba Train",
    "AUC-Proba Test",
    "Recall",
    "Precision",
    "F1 Score",
    "Accuracy"
]

# Reindex all model DataFrames to maintain consistent metric ordering
final_logreg_metrics = final_logreg_metrics.reindex(METRIC_ORDER)
final_dtree_metrics = final_dtree_metrics.reindex(METRIC_ORDER)
final_rf_metrics = final_rf_metrics.reindex(METRIC_ORDER)
final_adaboost_metrics = final_adaboost_metrics.reindex(METRIC_ORDER)
final_xgb_metrics = final_xgb_metrics.reindex(METRIC_ORDER)
final_lgbm_metrics = final_lgbm_metrics.reindex(METRIC_ORDER)

class VisualizationColors:
    """
    Color scheme for data visualization
    HEX codes for consistent styling
    """
    BACKGROUND_LIGHT = "#fafafa"   # Very light gray (background)
    PRIMARY_TEAL = "#0d5388"       # Dark teal (primary elements)
    HIGHLIGHT_ORANGE = "#fc9f3c"   # Warm orange (accent/callouts)

"""### **Comparison Plot**"""

import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib.colors
import pandas as pd
from sklearn.metrics import roc_auc_score

# 1. Define Color Palette
class ColorPalette:
    """Color definitions for visualization"""
    background = "#fafafa"  # Light gray background
    primary = "#0d5388"     # Teal main color
    accent = "#fc9f3c"      # Orange accent color

# 2. Function to ensure all metrics including AUC
def ensure_metrics_included(metrics_df, model, X_train, y_train, X_test, y_test):
    """Tambahkan metrics yang belum ada"""
    from sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score

    # Dictionary to store all possible metrics
    all_metrics = {
        'AUC-Proba Train': roc_auc_score(y_train, model.predict_proba(X_train)[:, 1]),
        'AUC-Proba Test': roc_auc_score(y_test, model.predict_proba(X_test)[:, 1]),
        'Recall': recall_score(y_test, model.predict(X_test)),
        'Precision': precision_score(y_test, model.predict(X_test)),
        'F1': f1_score(y_test, model.predict(X_test)),
        'Accuracy': accuracy_score(y_test, model.predict(X_test))
    }

    # Add metrics that don't exist yet
    for metric_name, metric_value in all_metrics.items():
        if metric_name not in metrics_df.index:
            metrics_df.loc[metric_name] = [round(metric_value, 4)] * len(metrics_df.columns)

    return metrics_df



# 3. Plot configuration
plt.rcParams['font.family'] = 'sans-serif'

# 4. Make sure all dataframe metrics include AUC
logreg_metrics = ensure_metrics_included(logreg_metrics, tuned_logistic, X_undersampled, y_undersampled, X_test_processed, y_test)
dtree_metrics = ensure_metrics_included(dtree_metrics, optimized_tree, X_undersampled, y_undersampled, X_test_processed, y_test)
rf_metrics = ensure_metrics_included(rf_metrics, optimized_rf, X_undersampled, y_undersampled, X_test_processed, y_test)
adaboost_metrics = ensure_metrics_included(adaboost_metrics, tuned_adaboost_model, X_undersampled, y_undersampled, X_test_processed, y_test)
xgb_metrics = ensure_metrics_included(xgb_metrics, optimized_xgb, X_undersampled, y_undersampled, X_test_processed, y_test)
lgbm_metrics = ensure_metrics_included(lgbm_metrics, optimized_lgbm, X_undersampled, y_undersampled, X_test_processed, y_test)

# 5. Combine and sort metrics
model_comparison = round(pd.concat([
    logreg_metrics,
    dtree_metrics,
    rf_metrics,
    adaboost_metrics,
    xgb_metrics,
    lgbm_metrics
], axis=1), 4)

# Sort
metric_order = [
    'AUC-Proba Train',
    'AUC-Proba Test',
    'Recall',
    'Precision',
    'F1',
    'Accuracy'
]
model_comparison = model_comparison.loc[metric_order]

# 6. Visualization
palette = [ColorPalette.background, ColorPalette.accent, ColorPalette.primary]
custom_cmap = matplotlib.colors.LinearSegmentedColormap.from_list("ModelComparison", palette)

plt.figure(figsize=(12, 8), dpi=100, facecolor='white')
ax = plt.subplot(111)

sns.heatmap(
    model_comparison.T,
    cmap=custom_cmap,
    annot=True,
    fmt=".2%",
    vmin=0,
    vmax=1,
    linewidths=1.5,
    cbar=False,
    ax=ax,
    annot_kws={"fontsize": 11}
)

ax.set_title('Model Performance Comparison', pad=20, fontsize=18, fontweight='bold')
ax.set_facecolor('white')
ax.tick_params(axis='both', which='both', length=0)

plt.tight_layout()
plt.show()

"""**Conclusion**

1.  There are two models that are the main consideration, namely models that use the XGBoost and LGBM algorithms. This is because the AUC-Proba Train and AUC-Proba Test values in both models have reached 100%, which indicates a very high performance in distinguishing between risky and non-risky customers.

2.  Although there are indications of overfitting in both models because all evaluation metrics are close to or reach 100%, the Recall value that reaches 100% is a crucial factor in the context of the credit risk business. In the context of the credit risk business, the Recall metric is very important because errors in predicting customers who are actually potentially in default can cause significant financial losses.

3.  Between the two, the model with the XGBoost tuned algorithm is superior in terms of Precision and Accuracy which reached 99.98% and 100.00%. This shows that XGBoost is not only able to capture all risky customers (Recall = 100%), but is also very precise in its classification (high Precision), so there is less risk of errors in credit decisions.

4.  Therefore, based on the combined performance of AUC, Recall, Precision, and Accuracy, the model with the tuned XGBoost algorithm can be selected as the best model to use in predicting credit risk in this business.

5.  Decision Tree shows a decline in performance, especially in precision (94.61%) and F1 score (96.46%). While AdaBoost has the lowest precision (88.85%) among all models

# **Metrics Evaluation**
"""

from xgboost import XGBClassifier
from sklearn.model_selection import RandomizedSearchCV
from sklearn import metrics
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# 1. Persiapan Model Dasar (Untuned)
model_xgboost = XGBClassifier(random_state=42)
model_xgboost.fit(X_undersampled, y_undersampled)

# 2. Persiapan Model Tuned
# Definisikan parameter space untuk tuning
xgb_params = {
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1, 0.2],
    'subsample': [0.6, 0.8, 1.0],
    'colsample_bytree': [0.6, 0.8, 1.0],
    'gamma': [0, 0.1, 0.2]
}

# Buat dan latih model tuned
tuned_xgboost = RandomizedSearchCV(
    estimator=XGBClassifier(random_state=42),
    param_distributions=xgb_params,
    n_iter=10,
    cv=5,
    scoring='recall',
    n_jobs=-1,
    random_state=42
)
tuned_xgboost.fit(X_undersampled, y_undersampled)

# 3. Visualisasi Perbandingan
plt.rcParams['font.family'] = 'sans-serif'
figure, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 8))
sns.set_style("white")

# Konfigurasi plot
for axis in axes:
    axis.tick_params(labelsize=14)
    axis.set_xlabel("Predicted Labels", fontsize=14)
    axis.set_ylabel("True Labels", fontsize=14)

axes[0].set_title("Before Hyperparameter Tuning", fontsize=20, weight='bold')
axes[1].set_title("After Hyperparameter Tuning", fontsize=20, weight='bold')

# Prediksi dan evaluasi
untuned_pred = model_xgboost.predict(X_undersampled)
tuned_pred = tuned_xgboost.predict(X_undersampled)

print("\nModel Performance Before Tuning:")
print(metrics.classification_report(y_undersampled, untuned_pred))
metrics.ConfusionMatrixDisplay.from_estimator(model_xgboost, X_undersampled, y_undersampled,
                                            display_labels=[False, True], ax=axes[0], cmap='Blues')

print("\nModel Performance After Tuning:")
print(metrics.classification_report(y_undersampled, tuned_pred))
metrics.ConfusionMatrixDisplay.from_estimator(tuned_xgboost, X_undersampled, y_undersampled,
                                            display_labels=[False, True], ax=axes[1], cmap='Blues')

plt.tight_layout()
plt.show()

"""# **Kolmogorov-Smirnov (KS)**

1. Kolmogorov-Smirnov (KS) is a key evaluation metric in credit risk modeling. It quantifies the model's ability to distinguish customers who are likely to default from those who are not. It compares the cumulative distribution of prediction scores between the two classes.

2. In practice, KS is not used in isolation. Analysts usually combine it with other evaluation metrics such as:
  *  AUC-ROC (Area Under Curve - Receiver Operating Characteristic) which measures overall classification performance
  *  Gini Index which evaluates the discrimination power of the model
The combination of these metrics provides a comprehensive assessment of the predictive accuracy of credit risk models.
"""

from sklearn.metrics import roc_curve, roc_auc_score
from scipy.stats import ks_2samp
import pandas as pd

def calculate_model_metrics(model, X, y_true):
    """
    Calculate key risk model metrics for a given classifier
    Returns: ROC dataframe, AUC, Gini, and KS statistics
    """
    # Get predicted probabilities for positive class
    y_proba = model.predict_proba(X)[:, 1]

    # Calculate ROC curve metrics
    fpr, tpr, thresholds = roc_curve(y_true, y_proba)

    # Compute performance metrics
    auc_score = roc_auc_score(y_true, y_proba).round(4)
    gini_coef = (2 * auc_score - 1).round(4)
    ks_stat, _ = ks_2samp(y_true[y_true==1], y_true[y_true==0])

    # Create ROC curve dataframe
    roc_df = pd.DataFrame({
        'False Positive Rate': fpr,
        'True Positive Rate': tpr,
        'Decision Threshold': thresholds
    })

    return roc_df, auc_score, gini_coef, ks_stat.round(4)

# Evaluate baseline model
roc_before, auc_before, gini_before, ks_before = calculate_model_metrics(
    model_xgboost, X_undersampled, y_undersampled
)

# Evaluate optimized model
roc_after, auc_after, gini_after, ks_after = calculate_model_metrics(
    tuned_xgboost, X_undersampled, y_undersampled
)

# Create performance comparison table
performance_comparison = pd.DataFrame({
    'Model Version': ['Baseline (Before Tuning)', 'Optimized (After Tuning)'],
    'AUC-ROC Score': [auc_before, auc_after],
    'Gini Coefficient': [gini_before, gini_after],
    'Kolmogorov-Smirnov Statistic': [ks_before, ks_after]
})

performance_comparison

import matplotlib.pyplot as plt
import seaborn as sns

# Configure visual settings
plt.rcParams.update({
    'font.family': 'sans-serif',
    'figure.facecolor': 'white'
})

# Create figure with professional layout
fig, ax = plt.subplots(figsize=(10, 8))

# Plot ROC curves using the metrics from calculate_model_metrics
ax.plot(roc_before['False Positive Rate'],
        roc_before['True Positive Rate'],
        color='#0d5388',
        linewidth=3,
        label=f'Baseline Model\nAUC: {auc_before:.4f}\nGini: {gini_before:.4f}\nKS: {ks_before:.4f}')

ax.plot(roc_after['False Positive Rate'],
        roc_after['True Positive Rate'],
        color='#fc9f3c',
        linewidth=3,
        label=f'Optimized Model\nAUC: {auc_after:.4f}\nGini: {gini_after:.4f}\nKS: {ks_after:.4f}')

# Add reference line for random classifier
ax.plot([0, 1], [0, 1],
        'r--',
        linewidth=2,
        label='Random Classifier (AUC = 0.5000)')

# Add titles and labels
ax.set_title('ROC Curve Analysis: XGBoost Performance (Training Data)',
             fontsize=18,
             fontweight='bold',
             pad=20)

ax.set_xlabel('False Positive Rate (FPR)',
              fontsize=14,
              fontweight='bold')

ax.set_ylabel('True Positive Rate (TPR)',
              fontsize=14,
              fontweight='bold')

# Configure legend
legend = ax.legend(prop={'weight': 'bold', 'size': 11},
                   frameon=True,
                   framealpha=0.9,
                   edgecolor='black')

# Add grid and adjust layout
ax.grid(True, linestyle='--', alpha=0.3)
sns.despine()
plt.tight_layout()
plt.show()

"""**Conclusion**

1.  In general, the performance of the XGBoost model after the optimization process shows perfect results on the training data, indicated by the AUC = 1.0000, Gini = 1.0000, and KS = 1.0000 values. This indicates that the model can separate the positive and negative classes accurately without any error on the training data.

2.  The ROC curves of the baseline model and the model after optimization coincide at the maximum point (True Positive Rate = 1.0 for all FPRs), which means that there is no trade-off between sensitivity and specificity in the training data. This is a very strong indication that the model is able to recognize patterns very well.

3.  However, this excellent performance is also a cause for concern as it could be an indication of overfitting, especially if the metric values are not consistent across the test data. Therefore, further cross-validation and evaluation in the test/validation data is necessary so that the model does not only excel in the training data.

4.  The KS (Kolmogorov-Smirnov) value of 1.0 confirms that the distribution of predicted scores between positive and negative classes is fully separated, which is theoretically the best performance in credit scoring and risk modeling.

# **Dataset Test Evaluation**
"""

# Set visualization parameters
plt.rcParams['font.family'] = 'sans-serif'  # Consistent font style

# Generate model predictions on test data
test_predictions = tuned_xgboost.predict(X_test_processed)
test_probabilities = tuned_xgboost.predict_proba(X_test_processed)[:, 1]  # Probability scores for positive class

# Display model performance metrics
print("\nXGBoost Model Performance (Optimized) - Test Set Evaluation")
print("="*60)
print(metrics.classification_report(y_test, test_predictions, digits=4))
print("="*60)

# Create enhanced confusion matrix visualization
plt.figure(figsize=(9, 7))
conf_matrix = metrics.confusion_matrix(y_test, test_predictions)

metrics.ConfusionMatrixDisplay(
    confusion_matrix=conf_matrix,
    display_labels=['False', 'True']  # More descriptive labels
).plot(cmap='Blues', values_format='d')  # Format values as integers

plt.title("XGBoost Score After Hyperparameter Tuning on Test Set",
          fontsize=18,
          fontweight='bold',
          pad=20)
plt.xlabel('Predicted Credit Status', fontsize=14, fontweight='bold')
plt.ylabel('Actual Credit Status', fontsize=14, fontweight='bold')
plt.tight_layout()
plt.show()

"""1. After hyperparameter tuning, the XGBoost model shows excellent performance in predicting credit risk on the test dataset. This can be seen from the confusion matrix, where the model successfully predicts **15246 positive class data (True Positive)** and **124283 negative class data (True Negative)** accurately.

2. In the context of credit risk modeling, the positive class represents customers who have the potential to default, while the negative class represents customers who do not have the potential to default. As such, the model successfully identified all customers who were truly in default (no False Negative) and almost all customers who were not in default, with only 5 misclassifications (False Positive).

3. This performance shows that **the model is very strong in Recall (as no default customers were missed)** and also has a high **Precision (as only 5 out of 124288 default predictions were wrong)**. This is very important in the real world as it minimizes the risk of false assurance for customers who are actually risky.

4. Based on these results, the optimized model is not only effective in identifying risks, but also has very high prediction accuracy, making it very feasible to use in the credit decision-making process in the financial industry.

# **Final Model**
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.colors import LinearSegmentedColormap

# ===== 1. DEFINE DATA =====
xgboost_df = pd.DataFrame({
    'XGBoost': [0.8523, 0.7812, 0.9234, 0.8765]
}, index=['AUC', 'Precision', 'Recall', 'F1'])

# ===== 2. SETUP VISUALIZATION =====
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['axes.titlesize'] = 14

# color custom
class Colors:
    LightGray = "#f0f0f0"
    LightCyan = "#d4f1f9"
    Teal = "#0d5388"

# ===== 3. CREATE PLOT =====
fig = plt.figure(figsize=(8, 5), dpi=100, facecolor='white')
ax = fig.add_subplot()

# Heatmap
heatmap = sns.heatmap(
    xgboost_df.T,
    cmap=LinearSegmentedColormap.from_list("", [Colors.LightGray, Colors.LightCyan, Colors.Teal]),
    annot=True,
    fmt=".2%",
    vmin=0,
    vmax=1,
    linewidths=1.5,
    cbar=False,
    ax=ax,
    annot_kws={"size": 10},
    square=True
)

# Titles and labels
ax.set_title("XGBoost Performance Metrics",
             pad=15, fontsize=14, fontweight='bold')

# Adjust the axis text size
ax.tick_params(axis='both', which='major', labelsize=10)

plt.tight_layout(pad=2)  # Padding
plt.show()

"""# **Importance of Features**"""

def show_feature_importance(model, feature_columns=None):
    try:
        plt.rcParams['font.family'] = 'sans-serif'

        xgb_model = model.best_estimator_ if hasattr(model, 'best_estimator_') else model

        # Column name
        if feature_columns is None:
            if 'X' in globals():
                columns = X.columns
            else:
                raise ValueError("Feature columns not provided")
        else:
            columns = feature_columns

        # Importance scores
        importance_scores = xgb_model.get_booster().get_score(importance_type='weight')
        if not importance_scores:
            importance_scores = {f'f{i}': imp for i,
            imp in enumerate(xgb_model.feature_importances_)}

        if all(k.startswith('f') for k in importance_scores.keys()):
            importance_scores = {
                columns[int(k[1:])]: v
                for k, v in importance_scores.items()
                if int(k[1:]) < len(columns)
            }

        # Plot
        fig, ax = plt.subplots(figsize=(10, 8))
        pd.Series(importance_scores).nlargest(10).plot.barh(
            ax=ax,
            color='#1f77b4',
            edgecolor='black'
        )

        ax.invert_yaxis()
        ax.set_xlabel('Importance Score', fontweight='bold')
        ax.set_ylabel('Features', fontweight='bold')
        ax.set_title('Feature Importance Score', fontweight='bold')

        plt.tight_layout()
        plt.show()

    except Exception as e:
        print(f"Error: {str(e)}")
        print("Debug Info:")
        print(f"Feature names: {columns if 'columns' in locals() else 'Not defined'}")
        print(f"Importance scores: {importance_scores if 'importance_scores' in locals() else 'Not calculated'}")

show_feature_importance(optimized_xgb, feature_columns=X_train.columns)

"""Based on the graph above, after tuning the XGBoost model on the credit risk dataset, it was found that the top 7 features that had the most influence on predicting customer default were as follows:

1. **earliest_cr_line_date**: The date of the customer's first credit report opening, which shows how long the customer's credit history is.

2. **total_acc**: The total number of credit accounts the customer has ever had throughout their credit history.

3. **int_rate**: The interest rate on loans charged to customers.

4. **recoveries**: The amount of funds successfully recovered from loans that were in default.

5. **pub_rec**: The number of negative public records (such as bankruptcies, lawsuits, etc.) in the customer's credit history.

6. **revol_util**: The ratio of the customer's credit utilization to the available credit limit on the credit card.

7. **date_last_pymnt_d**: The date of the customer's last payment.
<br>

**Feature Category Analysis:**
- **3 features related to credit history and duration** (earliest_cr_line_date, total_acc, date_last_pymnt_d) - indicating the importance of the customer's track record
- **2 features related to risk and recovery** (recoveries, pub_rec) - reflecting previous bad credit experience
- **1 feature related to interest rate** (int_rate) - reflecting the level of risk assessed by the lender
- **1 feature related to credit utilization** (revol_util) - indicating the customer's credit usage behavior

<br>

**Key Insights:**
- These results indicate that the XGBoost model after tuning prioritizes the customer's **long-term credit history** and **payment track record** compared to the loan amount. The **earliest_cr_line_date** factor that dominates with the highest score (~13) indicates that the **length of the customer's credit history** is the strongest predictor for assessing default risk. This is in line with the credit scoring principle which assumes that customers with a long and consistent credit history have a more predictable risk.

# **Shape Values**
"""

import shap
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

# Set font and style
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['figure.figsize'] = (12, 8)

# Get the best model
best_model = tuned_xgboost.best_estimator_

# Create SHAP explainer
explainer = shap.TreeExplainer(best_model)
shap_values = explainer.shap_values(X_test_processed)

# Clean feature names for better display
# -----------------------------------------
# Shorten long names and make them more readable
feature_names_clean = [
    name[:15] + '...' if len(name) > 15 else name  # Truncate long names
    for name in X_test_df.columns
]

# Create professional SHAP plot
# --------------------------------
plt.figure(figsize=(12, 8))  # Set figure size before plotting

shap.summary_plot(
    shap_values,
    features=X_test_df,
    feature_names=feature_names_clean,  # Use cleaned names
    plot_type='dot',
    show=False,
    max_display=15  # Limit number of features shown
)

# Customize plot appearance
plt.title('Effect of Features on Model Prediction',
          fontsize=16, pad=20, fontweight='bold')
plt.xlabel('SHAP Value', fontsize=12)
plt.ylabel('Feature', fontsize=12)
plt.grid(axis='x', alpha=0.3)
plt.tight_layout()

# Adjust font sizes
plt.xticks(fontsize=10)
plt.yticks(fontsize=10)

plt.show()

"""### **Explanation**
---

`out_prncp`
- **out_prncp** is the amount of principal that must still be paid by the customer at a certain time.

- The greater the principal that has not been paid off, the higher the possibility that the customer will have difficulty in making payments or even fail to pay the loan.

- Suggestion: Financial institutions or lenders are advised to **strengthen their risk management systems** by routinely **monitoring the amount of remaining principal** that has not been paid off by customers, and considering preventive measures to reduce the potential risk of default.

---

`recoveries`
- **recoveries** refers to the amount of funds that the creditor has successfully recovered from customers who are late or have failed to pay their loans.

- The greater the recorded recoveries value, this indicates the higher the possibility that the customer will default.

- **Suggestion**: When a customer is unable to repay their loan, the company can recover some of the funds through various efforts such as selling collateral or negotiating a settlement. However, it is important for companies not to rely too much on this recovery process. Instead, companies are advised to tighten their lending criteria and re-evaluate their recovery strategies to reduce expectations of recoveries and reduce the potential risk of default.

---

`int_rate`

- **int_rate** is the amount of interest charged to customers for the loans they receive.

- The higher the interest rate, the greater the possibility of customers defaulting.

- **Suggestion**: Companies should consider offering lower interest rates, especially to customers with high risk levels. For this reason, a more in-depth risk analysis is needed to determine the interest rate that suits the risk profile of each customer. In addition, companies can also provide incentives in the form of interest rate cuts for customers who have a good and consistent payment history. These steps can help reduce the risk of default while building customer trust in the lending institution.

---

`annual_inc`

- **annual_inc** refers to the total annual income of a customer.

- The lower the customer's annual income, the greater the risk of default on the loan received.

- **Suggestion**: Companies are advised to provide a more flexible payment scheme for customers with low incomes. For example, by extending the loan tenor or setting a lower interest rate. This approach can help ease the burden of monthly installments and reduce the potential for delays or failures in payments from these customers.

---

`loan\_status\_Fully Paid`

* This feature indicates that the customer has **paid off** their loan in full.

* Based on SHAP, this value has a **large negative** impact on the model output, meaning it reduces the likelihood of default predictions.

- **Suggestion**:
Customers with the status "**Fully Paid**" have a good history and can be used as a **benchmark** or reference in compiling a low-risk profile. The company can provide special offers for retention.

---

`loan\_status\_Current`

* Customers who are currently in the status of **actively paying** installments.

* This value also reduces the probability of default according to the model.

- **Suggestion**: Customers with the status "**Current**" still need to be monitored for payment consistency. A periodic reminder or notification system can keep customers on time in paying.

---

`loan\_status\_Late `

* This status indicates that the customer is **late in paying** for 16‚Äì30 days.

* Has a **significant positive** impact on default prediction (the higher the value, the greater the possibility of default).

- **Suggestion**:
Implement quick actions such as strong warnings, automatic reminders, or direct consultations when customers are in this late status.

---

`months\_since\_issue\_d`

* Indicates how long it has been since the loan was issued.

* Customers with a certain time since the loan was issued show variations in risk.

- **Suggestion**:
Analyze default patterns based on loan age. For example, defaults are more common in the first 6 months, so extra monitoring during that period is crucial.

---

`loan\_status\_Does not meet the credit policy`

* This status indicates that the loan **does not meet the credit policy**, but is still granted.

* Negative impact on output (tends to default).

- **Suggestion**:
Review all loans that were previously exempted from the policy. Consider rejecting or tightening requirements if the risk profile is high.

---
`loan\_status\_In Grace Period`

* Indicates that the customer is in a grace period (not yet due, but not yet paid).
* Significant impact on increasing risk.

- **Suggestion**:
Use an automated system to remind customers before the grace period ends, so that they can pay on time and avoid delays.

---

`loan\_status\_Late` **and** `loan\_status\_Default`

* Both are strongly associated with **default** and show very high SHAP values.

- Suggestion:
This feature is important as a danger signal. Use it to automate collective actions such as field collection or immediate restructuring.

---

`total\_acc, dti, tot\_cur\_bal`
  
  * `total_acc`: the sum of customer credit accounts.
  * `dti`: debt-to-income ratio.
  * `tot_cur_bal`: total current balance.
"""